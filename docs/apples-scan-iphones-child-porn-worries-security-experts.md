# 苹果扫描 iPhones 中儿童色情内容的计划令一些安全专家担忧

> 原文：<https://www.xda-developers.com/apples-scan-iphones-child-porn-worries-security-experts/>

苹果公司最近宣布了几个新的儿童安全功能，用于其所有的软件平台。新功能将于今年晚些时候在美国推出 [iOS 15](https://www.xda-developers.com/ios-15/) 、 [iPadOS 15](https://www.xda-developers.com/ipados-15-beta-1-hands-on/) 、 [watchOS 8](https://www.xda-developers.com/apple-watch-watchos-8-update/) 和 [macOS Monterey](https://www.xda-developers.com/how-to-install-macos-monterey/) ，它们旨在限制儿童性虐待材料(CSAM)的传播，以及其他内容。其中一项新功能主要是扫描 CSAM 的 iPhones 和 iPads，并将它们报告给国家失踪和被剥削儿童中心(NCMEC)。尽管苹果[声称](https://www.apple.com/child-safety/)其检测已知 CSAM *的方法“在设计时考虑到了用户隐私***”，但这引起了安全专家的担忧。**

 **根据最近*金融时报* [的报道](https://www.ft.com/content/14440f81-d405-452f-97e2-a81458f5411f)，安全研究人员警告说，苹果的新工具可能被滥用于监视，将数百万人的个人信息置于危险之中。他们的担忧是基于苹果本周早些时候与一些美国学者分享的数据。两名未透露姓名的安全研究人员参加了苹果的简报会，他们透露，如果这个名为“neuralMatch”的拟议系统在 iPhone 或 iPad 上检测到 CSAM，它将主动提醒一组人工审查人员。如果审查人员能够核实材料，他们将联系执法部门。

尽管安全研究人员支持苹果限制 CSAM 传播的努力，但一些人担心这一工具可能被政府滥用来获取公民数据。剑桥大学安全工程教授罗斯·安德森说:*“这绝对是一个骇人听闻的想法，因为它将导致分布式大规模监控...我们的手机和笔记本电脑。”*约翰·霍普金斯信息安全学院的计算机科学教授马修·格林也[在推特上提出了他的担忧](https://twitter.com/matthew_d_green/status/1423071186616000513)，他写道:

> 但是即使你相信苹果不会允许这些工具被滥用...还是有很多要担心的。这些系统依赖于一个“有问题的媒体散列”的数据库，作为一个消费者，你不能查看...哈希使用苹果公司开发的一种新的专有神经哈希算法，并得到了 NCMEC 的同意...我们对这个算法了解不多。如果有人能制造碰撞。

尽管该算法目前被训练用于识别 CSAM，但它也可以用于扫描其他目标图像或文本，如反政府标志，这使其成为专制政府的一个非常有用的工具。苹果的先例也可能迫使其他科技巨头提供类似的功能，这可能会导致一场隐私噩梦。

苹果尚未对这些担忧做出回应。公司一发布声明，我们就会更新这篇文章。有关 CSAM 检测功能的更多信息，请点击[此链接](https://www.apple.com/child-safety/pdf/CSAM_Detection_Technical_Summary.pdf)。**