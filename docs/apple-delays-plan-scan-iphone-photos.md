# 苹果推迟扫描 iPhone 照片中虐童图像的计划

> 原文：<https://www.xda-developers.com/apple-delays-plan-scan-iphone-photos/>

上个月初，苹果宣布为其所有软件平台增加几项新的儿童安全功能。其中一项新功能旨在扫描 iPhone 和 iPad 照片，寻找儿童性虐待材料(CSAM)。一些安全专家对这项功能表示担忧，认为政府可能会利用它来获取公民的数据。尽管苹果最初声称其检测已知 CSAM 的方法不会对用户隐私构成威胁，但该公司现在已经推迟了发布时间。

在最近的一份声明中(via [*9to5Mac*](https://9to5mac.com/2021/09/03/apple-delays-csam-detection-feature/) ，苹果表示:

上个月，我们宣布了一些功能计划，旨在帮助保护儿童免受利用通信工具招募和剥削他们的掠夺者的侵害，并限制儿童性虐待材料的传播。根据客户、倡导团体、研究人员和其他人的反馈，我们决定在未来几个月花更多时间收集意见并做出改进，然后再发布这些至关重要的儿童安全功能。”

苹果之前计划发布新的儿童安全功能，作为 iOS 15 、 [iPadOS 15](https://www.xda-developers.com/ipados-15-beta-1-hands-on/) 和 [macOS Monterey](https://www.xda-developers.com/how-to-install-macos-monterey/) 更新的一部分。然而，今年晚些时候推出的更新将不包括新的儿童安全功能。目前，苹果公司还没有分享推出的暂定时间表，也没有提供任何关于它计划做出的改变以解决隐私问题的细节。

简单来说，苹果的 CSAM 扫描功能目前可以将你 iPhone 或 iPad 上的照片与儿童安全组织提供的已知 CSAM 图片数据库进行匹配。如果它在设备上检测到任何 CSAM，它可以主动提醒一组人工审查人员，他们可以在验证材料后联系执法部门。安全研究人员认为，尽管该算法目前被训练用于检测 CSAM，但它可以适用于扫描其他图像或文本，使其成为威权政府的一个宝贵工具。