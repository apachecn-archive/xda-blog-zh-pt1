<html>
<head>
<title>Android's NNAPI now supports hardware-acceleration with PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Android的NNAPI现在支持PyTorch的硬件加速</h1>
<blockquote>原文：<a href="https://www.xda-developers.com/android-neural-networks-api-nnapi-hardware-accelerated-inferencing-pytorch/#0001-01-01">https://www.xda-developers.com/android-neural-networks-api-nnapi-hardware-accelerated-inferencing-pytorch/#0001-01-01</a></blockquote><div><header class="article_heading">


<h1 class="heading_title">Android的神经网络API现在支持脸书PyTorch框架的硬件加速推理</h1>

<p class="heading_sharing">  </p>


<p class="heading_excerpt">Android的神经网络API (NNAPI)现在支持脸书PyTorch框架的硬件加速推理。请继续阅读了解更多信息！</p>



  </header>

<section id="article-body" class="article-body" itemprop="articleBody">

<div class="content-block-regular">
<p/><p>机器学习在许多方面塑造了我们的现在，以至于我们甚至不再注意到它。以前不可能完成的任务现在已经变得轻而易举，使得这项技术及其好处能够更广泛地为大众所接受。很多这一切都是通过设备上的机器学习和谷歌的神经网络API (NNAPI)实现的。现在，随着Android团队宣布支持一个原型功能，使开发人员能够通过脸书的PyTorch框架使用硬件加速推理，更多的用户将能够体验到加速神经网络及其好处。</p>

  
<p>设备上的机器学习允许机器学习模型在设备上本地运行，而不需要将数据传输到服务器，从而实现更低的延迟、更好的隐私性和更好的连接性。Android神经网络API (NNAPI)旨在为Android设备上的机器学习运行计算密集型操作。NNAPI提供了一组API，以从可用的硬件加速器(包括GPU、DSP和npu)中获益。</p>

<p>NNAPI可以通过Android C API直接访问，也可以通过更高级别的框架访问，比如<a href="https://www.xda-developers.com/tensorflow-lite-mobile-gpu-android/"> TensorFlow Lite </a>。根据今天的公告，<a href="https://ai.facebook.com/tools/pytorch/"> PyTorch Mobile </a>宣布了一个支持NNAPI的新原型功能，从而使开发人员能够在PyTorch框架中使用硬件加速推理。这个初始版本包括对Android 10及以上版本上众所周知的线性卷积和多层感知器模型的支持。使用MobileNetV2模型进行的性能测试显示，与单线程CPU相比，速度提高了10倍。作为向完全稳定版本发展的一部分，未来的更新将包括对其他操作符和模型架构的支持，包括Mask R-CNN，一种流行的对象检测和实例分割模型。</p>

<p>或许在PyTorch之上构建的最知名的软件是特斯拉的自动驾驶软件。虽然今天的公告没有为Autopilot带来任何直接消息，但它确实为数百万使用基于PyTorch构建的软件的Android用户开放了加速神经网络的好处。</p>

 </div>


<p id="article-waypoint"/>
</section>


        </div>    
</body>
</html>